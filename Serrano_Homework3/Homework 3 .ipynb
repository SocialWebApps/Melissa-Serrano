{
 "metadata": {
  "name": "",
  "signature": "sha256:18b00eb9e0ff2bb379fc843e132f9784e5f85daa35595a506ddcc41d36cc90e2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Homework 3\n",
      "\n",
      "##Melissa Serrano\n",
      "Assignment 3:\n",
      "\n",
      "Study the code for LinkedIn and use it for your own account and print out results (you can change code so only some of the data is printed out -- just to verify that the step worked). Ensure that all the graphics get plotted (I did not finish that in the class; but I have given a way in the discussion on Github; you may know better ways).\n",
      "\n",
      "Look over my output and find ways to achieve one of the following for your code. Test out for your own data:\n",
      "\n",
      "1. Do a deep search so even past affiliations (in  the profiles of networked professionals) can be included in the frequency listing - recall my comments about Motorola engineers; \n",
      "2. **Look at the professional titles - normalize to a minimum (CTO, Founder, Engineer, Programmer, Professor, and Student. Suggest some other common ones) and print out the updated list; and **\n",
      "3. Look at the tokens - normalize them to a meaningful subset  (drop items such as 'and' ) and they should include 'details' of their roles (simillar to the first name - asst/assoc/full professor, undergrad/grad student, senior/mid-level/junior engineer, etc). How will you determine that? An easy rule might be the number of years in the profession - include their graduate work also. Just come up with a rule to do this normalization and apply it uniformly. How will you apply to CTO/CEO - a similar idea will do.\n",
      "\n",
      "This assignment has been adapted from the examples in Chapter 3 from [_Mining the Social Web (2nd Edition)_](http://bit.ly/135dHfs). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "First use LinkedIn OAuth credentials to receive an access token suitable for development and accessing your own data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obtain your OAuth 2.0 credentials from https://www.linkedin.com/secure/developer and use the four credentials (\"API Key\", \"Secret Key\", \"OAuth User Token\", and \"OAuth User Secret\") below to begin accessing LinkedIn data.\n",
      "\n",
      "Before trying to access LinkedIn, be sure to run the following command in your Anaconda environment: \n",
      "<code> $ pip install python-linkedin </code>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from linkedin import linkedin # pip install python-linkedin\n",
      "\n",
      "# Define CONSUMER_KEY, CONSUMER_SECRET,  \n",
      "# USER_TOKEN, and USER_SECRET from the credentials \n",
      "# provided in your LinkedIn application\n",
      "\n",
      "CONSUMER_KEY = '78p7hn01vsqh35'\n",
      "CONSUMER_SECRET = '0cCtHW5NskYCKYNO'\n",
      "USER_TOKEN = 'f62a23ab-735d-4a98-b014-e7e206846cbb'\n",
      "USER_SECRET = '05d20d59-d61c-4f0e-b361-947a0b1da352'\n",
      "RETURN_URL = '' # Not required for developer authentication\n",
      "\n",
      "# Instantiate the developer authentication class\n",
      "auth = linkedin.LinkedInDeveloperAuthentication(CONSUMER_KEY, CONSUMER_SECRET, \n",
      "                                USER_TOKEN, USER_SECRET, \n",
      "                                RETURN_URL, \n",
      "                                permissions=linkedin.PERMISSIONS.enums.values())\n",
      "\n",
      "# Pass it in to the app...\n",
      "app = linkedin.LinkedInApplication(auth)\n",
      "\n",
      "# Use the app...\n",
      "#Since app was set to the LinkedIn App of the Developer Authentication and credentials \n",
      "#the profile retrieved will be that of the developer\n",
      "app.get_profile()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "{u'firstName': u'Melissa',\n",
        " u'headline': u'Computer Science Major at FAU',\n",
        " u'id': u'g4aNPiE7Kg',\n",
        " u'lastName': u'Serrano',\n",
        " u'siteStandardProfileRequest': {u'url': u'https://www.linkedin.com/profile/view?id=284926868&authType=name&authToken=Ziv7&trk=api*a4043814*s4109794*'}}"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Retrieving your LinkedIn connections and storing them to disk"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To avoid accessive calls to the API we will connect to the API and store the data we want to analyze on our local disk.  I saved it in a relative directory for easy access from here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "connections = app.get_connections()\n",
      "\n",
      "#connections_data = 'C:\\\\Users\\\\MisaelNMelissa\\\\Dropbox\\\\Mel_School\\\\Web 2.0 Architectures & Algorithms\\\\Homework\\\\Homework3_Serrano\\\\resources\\\\connections.json'\n",
      "connections_data = 'resources\\\\connections.json'\n",
      "f = open(connections_data, 'w')\n",
      "f.write(json.dumps(connections, indent=1))\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can reuse the data without using the API later like this...\n",
      "connections = json.loads(open(connections_data).read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Execute this cell if you need to reload data from the file...\n",
      "import json\n",
      "connections = json.loads(open('resources\\\\connections.json').read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Pretty-printing your LinkedIn connections' data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below we use prettytable to neatly print raw connections' data.  Later we will normalize the locations since some locations  use the terms 'Greater' and 'Area' - Even in places where it doesn't seem to quite make sense."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from prettytable import PrettyTable # pip install prettytable\n",
      "\n",
      "pt = PrettyTable(field_names=['Name', 'Location'])\n",
      "pt.align = 'l'\n",
      "\n",
      "[ pt.add_row((c['firstName'] + ' ' + c['lastName'], c['location']['name'])) \n",
      "  for c in connections['values']\n",
      "      if c.has_key('location')]\n",
      "\n",
      "print pt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+----------------------+-----------------------------------------------+\n",
        "| Name                 | Location                                      |\n",
        "+----------------------+-----------------------------------------------+\n",
        "| Jim Devine           | West Palm Beach, Florida Area                 |\n",
        "| Jo-Ann Calcagno      | West Palm Beach, Florida Area                 |\n",
        "| Busra Demirci        | West Palm Beach, Florida Area                 |\n",
        "| Gustavo M\u00f3naco       | Orlando, Florida Area                         |\n",
        "| Alton Gaines         | Miami/Fort Lauderdale Area                    |\n",
        "| Keith Haizlett       | Miami/Fort Lauderdale Area                    |\n",
        "| Juanita Mainster     | Miami/Fort Lauderdale Area                    |\n",
        "| Marlow Charite       | West Palm Beach, Florida Area                 |\n",
        "| Jebin Shakya         | Miami/Fort Lauderdale Area                    |\n",
        "| Thomas Sonderman     | West Palm Beach, Florida Area                 |\n",
        "| Lauren Paino         | Greater New York City Area                    |\n",
        "| Mia Falzarano        | Greensboro/Winston-Salem, North Carolina Area |\n",
        "| David Vizcaino       | Miami/Fort Lauderdale Area                    |\n",
        "| Jimmy Mauri          | Miami/Fort Lauderdale Area                    |\n",
        "| Maria Bojko          | West Palm Beach, Florida Area                 |\n",
        "| Kevin Medina         | Greater Milwaukee Area                        |\n",
        "| Santiago Aguerrevere | Miami/Fort Lauderdale Area                    |\n",
        "| Debra Lewis          | West Palm Beach, Florida Area                 |\n",
        "| cbaechle fau         | Albany, New York Area                         |\n",
        "| Veronica M           | Sacramento, California Area                   |\n",
        "| Crissy Olivas        | Wilmington, North Carolina Area               |\n",
        "| Aunt Barbara Leonard | United States                                 |\n",
        "+----------------------+-----------------------------------------------+\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Displaying job position history for your profile and a connection's profile"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "# See http://developer.linkedin.com/documents/profile-fields#fullprofile\n",
      "# for details on additional field selectors that can be passed in for\n",
      "# retrieving additional profile information.\n",
      "\n",
      "# Display my positions...\n",
      "\n",
      "my_positions = app.get_profile(selectors=['positions'])\n",
      "print json.dumps(my_positions, indent=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\n",
        " \"positions\": {\n",
        "  \"_total\": 3, \n",
        "  \"values\": [\n",
        "   {\n",
        "    \"startDate\": {\n",
        "     \"year\": 2014, \n",
        "     \"month\": 6\n",
        "    }, \n",
        "    \"endDate\": {\n",
        "     \"year\": 2014, \n",
        "     \"month\": 12\n",
        "    }, \n",
        "    \"title\": \"Software Engineer Intern\", \n",
        "    \"company\": {\n",
        "     \"id\": 2761, \n",
        "     \"name\": \"Emerson Network Power\"\n",
        "    }, \n",
        "    \"summary\": \"Optimize performance and readability by making code improvements to overly complex methods and applying appropriate error-handling. I apply Java best practices while refactoring code into smaller more specialized methods, reducing logic where possible, including appropriate logging, and ensuring consistent error handling.\\nTools used: JDeveloper, putty, WinSCP, SonarQube, Maven, SVN Tortoise, WebLogic Server\", \n",
        "    \"isCurrent\": false, \n",
        "    \"id\": 557507566\n",
        "   }, \n",
        "   {\n",
        "    \"startDate\": {\n",
        "     \"year\": 2009, \n",
        "     \"month\": 1\n",
        "    }, \n",
        "    \"endDate\": {\n",
        "     \"year\": 2010, \n",
        "     \"month\": 6\n",
        "    }, \n",
        "    \"title\": \"Partnership Specialist\", \n",
        "    \"company\": {\n",
        "     \"id\": 7811, \n",
        "     \"name\": \"U.S. Census Bureau\"\n",
        "    }, \n",
        "    \"summary\": \"Developed Partnerships with Local Governments, businesses, faith-based groups, social service organizations, civic groups, and other grassroots entities to spread the 2010 Census Awareness Campaign\\nTrained, directly supervised, assign tasks, review and signed timesheets for 4 Partnership Assistants\\nConducted workshops and trainings to support the 2010 Census Awareness Campaign\\nMade Presentations verbally and with Powerpoint or other visual presentation tools, in both English and Spanish\", \n",
        "    \"isCurrent\": false, \n",
        "    \"id\": 438159306\n",
        "   }, \n",
        "   {\n",
        "    \"startDate\": {\n",
        "     \"year\": 2006, \n",
        "     \"month\": 7\n",
        "    }, \n",
        "    \"endDate\": {\n",
        "     \"year\": 2009, \n",
        "     \"month\": 6\n",
        "    }, \n",
        "    \"title\": \"Seasonal Manager\", \n",
        "    \"company\": {\n",
        "     \"name\": \"Northlake Washbowl Inc\"\n",
        "    }, \n",
        "    \"summary\": \"Review all employee's daily logs of money spent and money received for discrepancies \\nTake inventory of all snacks, soaps, and sodas \\nReconcile sales against inventory \\nHandle all customer and employee relations and complaints \\nAdvertise and promote any company events and sales \\nCompute, classify, and record numerical data weekly into the QuickBooks program to keep financial records complete\", \n",
        "    \"isCurrent\": false, \n",
        "    \"id\": 438159308\n",
        "   }\n",
        "  ]\n",
        " }\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Display positions for the first connection in my network...\n",
      "\n",
      "# Get an id for a connection. We'll just pick the first one.\n",
      "connection_id = connections['values'][0]['id']\n",
      "connection_positions = app.get_profile(member_id=connection_id, \n",
      "                                       selectors=['positions'])\n",
      "print json.dumps(connection_positions, indent=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\n",
        " \"positions\": {\n",
        "  \"_total\": 2, \n",
        "  \"values\": [\n",
        "   {\n",
        "    \"startDate\": {\n",
        "     \"year\": 2013, \n",
        "     \"month\": 1\n",
        "    }, \n",
        "    \"title\": \"Advisory Board Member\", \n",
        "    \"company\": {\n",
        "     \"name\": \"Gold 2 Good\"\n",
        "    }, \n",
        "    \"summary\": \"Gold 2 Good is a firm that facilitates not-for profit fundraising through the process of assisting the organizations with events that convert unwanted donor jewelry into cash for needed projects.\", \n",
        "    \"isCurrent\": true, \n",
        "    \"id\": 441425230\n",
        "   }, \n",
        "   {\n",
        "    \"startDate\": {\n",
        "     \"year\": 2011, \n",
        "     \"month\": 12\n",
        "    }, \n",
        "    \"title\": \"Marketing Director\", \n",
        "    \"company\": {\n",
        "     \"id\": 979756, \n",
        "     \"name\": \"Exceleron Designs\"\n",
        "    }, \n",
        "    \"summary\": \"Marketing Agency\", \n",
        "    \"isCurrent\": true, \n",
        "    \"id\": 212547857\n",
        "   }\n",
        "  ]\n",
        " }\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Using field selector syntax to request additional details for APIs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# See http://developer.linkedin.com/documents/understanding-field-selectors\n",
      "# for more information on the field selector syntax\n",
      "\n",
      "#List my positions start and end date\n",
      "#Within the position selector is the capability to list\n",
      "#comany names, locations, industry, and id where available by using the field selector\n",
      "my_positions = app.get_profile(selectors=['positions:(start-date,end-date,company:(name,locations,industry,id))'])\n",
      "print json.dumps(my_positions, indent=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\n",
        " \"positions\": {\n",
        "  \"_total\": 3, \n",
        "  \"values\": [\n",
        "   {\n",
        "    \"startDate\": {\n",
        "     \"year\": 2014, \n",
        "     \"month\": 6\n",
        "    }, \n",
        "    \"company\": {\n",
        "     \"id\": 2761, \n",
        "     \"name\": \"Emerson Network Power\"\n",
        "    }, \n",
        "    \"endDate\": {\n",
        "     \"year\": 2014, \n",
        "     \"month\": 12\n",
        "    }\n",
        "   }, \n",
        "   {\n",
        "    \"startDate\": {\n",
        "     \"year\": 2009, \n",
        "     \"month\": 1\n",
        "    }, \n",
        "    \"company\": {\n",
        "     \"id\": 7811, \n",
        "     \"name\": \"U.S. Census Bureau\"\n",
        "    }, \n",
        "    \"endDate\": {\n",
        "     \"year\": 2010, \n",
        "     \"month\": 6\n",
        "    }\n",
        "   }, \n",
        "   {\n",
        "    \"startDate\": {\n",
        "     \"year\": 2006, \n",
        "     \"month\": 7\n",
        "    }, \n",
        "    \"company\": {\n",
        "     \"name\": \"Northlake Washbowl Inc\"\n",
        "    }, \n",
        "    \"endDate\": {\n",
        "     \"year\": 2009, \n",
        "     \"month\": 6\n",
        "    }\n",
        "   }\n",
        "  ]\n",
        " }\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Simple normalization of company suffixes from address book data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First you need to go to http://www.linkedin.com/people/export-settings to export your connections data into a .csv file.  I saved it to a relative directory for easy access from here.\n",
      "\n",
      "I applied a transformation on a parent company and it's subsidiary company."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os # provides current path - I substituted with absolute path\n",
      "import csv\n",
      "from collections import Counter\n",
      "from operator import itemgetter\n",
      "from prettytable import PrettyTable\n",
      "\n",
      "# Place your \"Outlook CSV\" formatted file of connections from \n",
      "# http://www.linkedin.com/people/export-settings at the following location: \n",
      "# C:\\\\Users\\\\MisaelNMelissa\\\\Dropbox\\\\Mel_School\\\\Web 2.0 Architectures & Algorithms\\\\Homework\\\\Homework3_Serrano\\\\resources\\\\my_connections.csv\n",
      "\n",
      "CSV_FILE = \"resources\\\\my_connections.csv\"\n",
      "# Define a set of transforms that converts the first item\n",
      "# to the second item. Here, we're simply handling some\n",
      "# commonly known abbreviations, stripping off common suffixes, \n",
      "# etc.\n",
      "\n",
      "transforms = [(', Inc.', ''), (', Inc', ''), (', LLC', ''), (', LLP', ''),\n",
      "               (' LLC', ''), (' Inc.', ''), (' Inc', ''),\n",
      "               #After running this I noticed that you can't see the result of the transformation\n",
      "               #since my connections did not denote their companies in this manner\n",
      "               #However I did notice that I have connections which listed the parent company and others listed the subsidary company\n",
      "               #Although they all work at the same place (my previous internship)\n",
      "               #So I added a transformation for this\n",
      "               ('Avocent', 'Emerson Network Power'), ('Emerson Network Power', 'Emerson Network Power - Avocent')]\n",
      "\n",
      "csvReader = csv.DictReader(open(CSV_FILE), delimiter=',', quotechar='\"')\n",
      "contacts = [row for row in csvReader]\n",
      "companies = [c['Company'].strip() for c in contacts if c['Company'].strip() != '']\n",
      "\n",
      "for i, _ in enumerate(companies):\n",
      "    for transform in transforms:\n",
      "        companies[i] = companies[i].replace(*transform)\n",
      "\n",
      "pt = PrettyTable(field_names=['Company', 'Freq'])\n",
      "pt.align = 'l'\n",
      "c = Counter(companies)\n",
      "[pt.add_row([company, freq]) \n",
      " for (company, freq) in sorted(c.items(), key=itemgetter(1), reverse=True) \n",
      "     if freq > 0] # Try 0 - in order to see all the company variations\n",
      "print pt "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+----------------------------------------+------+\n",
        "| Company                                | Freq |\n",
        "+----------------------------------------+------+\n",
        "| Emerson Network Power - Avocent        | 3    |\n",
        "| Modernizing Medicine                   | 2    |\n",
        "| Blue ribbon cleaning serice            | 1    |\n",
        "| California Department of Public Health | 1    |\n",
        "| Ultimate Software                      | 1    |\n",
        "| Oxbow                                  | 1    |\n",
        "| MovieTickets.com                       | 1    |\n",
        "| U.S. Globe                             | 1    |\n",
        "| Gold 2 Good                            | 1    |\n",
        "| none                                   | 1    |\n",
        "| OpenPeak                               | 1    |\n",
        "| Epilepsy Foundation of Florida         | 1    |\n",
        "| STS Aviation Group                     | 1    |\n",
        "| Philips Healthcare                     | 1    |\n",
        "| Indian River State College             | 1    |\n",
        "+----------------------------------------+------+\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Standardizing common job titles and computing their frequencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result of this tranformation is not as relevant for my set of connections.  Since I don't have many connections with these types of titles.  \n",
      "\n",
      "One transformation of job titles that would be relevant might be to group similiar job titles such as Software Engineers of all levels (Intern, Associate, Junior, Senior).  Depending on the need this may be helpful or may not, but we'll demonstrate it below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "from operator import itemgetter\n",
      "from collections import Counter\n",
      "from prettytable import PrettyTable\n",
      "\n",
      "# Place your \"Outlook CSV\" formatted file of connections from \n",
      "# http://www.linkedin.com/people/export-settings at the following location: \n",
      "# C:\\\\Users\\\\MisaelNMelissa\\\\Dropbox\\\\Mel_School\\\\Web 2.0 Architectures & Algorithms\\\\Homework\\\\Homework3_Serrano\\\\resources\\\\my_connections.csv\n",
      "\n",
      "CSV_FILE = \"resources\\\\my_connections.csv\"\n",
      "transforms = [\n",
      "    ('Sr.', 'Senior'),\n",
      "    ('Sr', 'Senior'),\n",
      "    ('Jr.', 'Junior'),\n",
      "    ('Jr', 'Junior'),\n",
      "    ('CEO', 'Chief Executive Officer'),\n",
      "    ('COO', 'Chief Operating Officer'),\n",
      "    ('CTO', 'Chief Technology Officer'),\n",
      "    ('CFO', 'Chief Finance Officer'),\n",
      "    ('VP', 'Vice President'),('Visiting', ''), ('Assistant', '')\n",
      "    ]\n",
      "# added the last two - some entries of professors with these qualifications...\n",
      "# much more refinement is needed!\n",
      "csvReader = csv.DictReader(open(CSV_FILE), delimiter=',', quotechar='\"')\n",
      "contacts = [row for row in csvReader]\n",
      "\n",
      "# Read in a list of titles and split apart\n",
      "# any combined titles like \"President/CEO.\"\n",
      "# Other variations could be handled as well, such\n",
      "# as \"President & CEO\", \"President and CEO\", etc.\n",
      "\n",
      "titles = []\n",
      "for contact in contacts:\n",
      "    titles.extend([t.strip() for t in contact['Job Title'].split('/')\n",
      "                  if contact['Job Title'].strip() != ''])\n",
      "\n",
      "# Replace common/known abbreviations\n",
      "\n",
      "for i, _ in enumerate(titles):\n",
      "    for transform in transforms:\n",
      "        titles[i] = titles[i].replace(*transform)\n",
      "\n",
      "# Print out a table of titles sorted by frequency\n",
      "\n",
      "pt = PrettyTable(field_names=['Title', 'Freq'])\n",
      "pt.align = 'l'\n",
      "c = Counter(titles)\n",
      "[pt.add_row([title, freq]) \n",
      " for (title, freq) in sorted(c.items(), key=itemgetter(1), reverse=True) \n",
      "     if freq > 0]\n",
      "print pt\n",
      "\n",
      "# Print out a table of tokens sorted by frequency\n",
      "\n",
      "tokens = []\n",
      "for title in titles:\n",
      "    tokens.extend([t.strip(',') for t in title.split()])\n",
      "pt = PrettyTable(field_names=['Token', 'Freq'])\n",
      "pt.align = 'l'\n",
      "c = Counter(tokens)\n",
      "[pt.add_row([token, freq]) \n",
      " for (token, freq) in sorted(c.items(), key=itemgetter(1), reverse=True) \n",
      "     if freq > 0 and len(token) > 2]\n",
      "print pt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+--------------------------------------+------+\n",
        "| Title                                | Freq |\n",
        "+--------------------------------------+------+\n",
        "| Application Developer                | 1    |\n",
        "| Database Administrator               | 1    |\n",
        "| Vice President Operations            | 1    |\n",
        "| Engineering Talent Manager           | 1    |\n",
        "| Design and Technology Coordinator    | 1    |\n",
        "| Advisory Board Member                | 1    |\n",
        "|  Accounting Manager                  | 1    |\n",
        "| QA                                   | 1    |\n",
        "| Senior Director, Enterprise DevOps   | 1    |\n",
        "| Scientist                            | 1    |\n",
        "| Software Engineer Intern             | 1    |\n",
        "| Associate Quality Assurance Engineer | 1    |\n",
        "| Manager of a cleaning company        | 1    |\n",
        "| Advisor                              | 1    |\n",
        "| none                                 | 1    |\n",
        "| Associate Software Engineer          | 1    |\n",
        "| Software Development Engineer        | 1    |\n",
        "| Certified Healthcare Navigator       | 1    |\n",
        "+--------------------------------------+------+\n",
        "+---------------+------+\n",
        "| Token         | Freq |\n",
        "+---------------+------+\n",
        "| Engineer      | 4    |\n",
        "| Manager       | 3    |\n",
        "| Software      | 3    |\n",
        "| Associate     | 2    |\n",
        "| and           | 1    |\n",
        "| Healthcare    | 1    |\n",
        "| Member        | 1    |\n",
        "| Board         | 1    |\n",
        "| Assurance     | 1    |\n",
        "| Development   | 1    |\n",
        "| Intern        | 1    |\n",
        "| Administrator | 1    |\n",
        "| Database      | 1    |\n",
        "| DevOps        | 1    |\n",
        "| Director      | 1    |\n",
        "| Advisor       | 1    |\n",
        "| President     | 1    |\n",
        "| Certified     | 1    |\n",
        "| Talent        | 1    |\n",
        "| Vice          | 1    |\n",
        "| company       | 1    |\n",
        "| cleaning      | 1    |\n",
        "| Enterprise    | 1    |\n",
        "| Senior        | 1    |\n",
        "| Technology    | 1    |\n",
        "| Operations    | 1    |\n",
        "| Advisory      | 1    |\n",
        "| none          | 1    |\n",
        "| Application   | 1    |\n",
        "| Coordinator   | 1    |\n",
        "| Engineering   | 1    |\n",
        "| Scientist     | 1    |\n",
        "| Design        | 1    |\n",
        "| Accounting    | 1    |\n",
        "| Navigator     | 1    |\n",
        "| Developer     | 1    |\n",
        "| Quality       | 1    |\n",
        "+---------------+------+\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Transform job titles to group similiar job titles such as Software Engineers of all levels (Intern, Associate, Junior, Senior).  Also combine Software Developer and Software Engineer since these titles are usually used interchangeabely.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "from operator import itemgetter\n",
      "from collections import Counter\n",
      "from prettytable import PrettyTable\n",
      "\n",
      "# Place your \"Outlook CSV\" formatted file of connections from \n",
      "# http://www.linkedin.com/people/export-settings at the following location: \n",
      "# C:\\\\Users\\\\MisaelNMelissa\\\\Dropbox\\\\Mel_School\\\\Web 2.0 Architectures & Algorithms\\\\Homework\\\\Homework3_Serrano\\\\resources\\\\my_connections.csv\n",
      "\n",
      "transforms = [\n",
      "    ('Sr.', 'Senior'),\n",
      "    ('Sr', 'Senior'),\n",
      "    ('Jr.', 'Junior'),\n",
      "    ('Jr', 'Junior'),\n",
      "    ('CEO', 'Chief Executive Officer'),\n",
      "    ('COO', 'Chief Operating Officer'),\n",
      "    ('CTO', 'Chief Technology Officer'),\n",
      "    ('CFO', 'Chief Finance Officer'),\n",
      "    ('VP', 'Vice President'),('Visiting', ''), ('Assistant', ''),\n",
      "#Group all Software Engineers\n",
      "    ('Software Engineer Intern', 'Software Engineer'),\n",
      "    ('Associate Software Engineer', 'Software Engineer'),\n",
      "    ('Junior Software Engineer', 'Software Engineer'),\n",
      "    ('Senior Software Engineer', 'Software Engineer'),\n",
      "    ('Software Development Engineer', 'Software Engineer'),\n",
      "#Group all Developers\n",
      "    ('Software Developer', 'Developer'),\n",
      "    ('Application Developer', 'Developer'),\n",
      "    ]\n",
      "# added the last two - some entries of professors with these qualifications...\n",
      "# much more refinement is needed!\n",
      "csvReader = csv.DictReader(open(CSV_FILE), delimiter=',', quotechar='\"')\n",
      "contacts = [row for row in csvReader]\n",
      "\n",
      "# Read in a list of titles and split apart\n",
      "# any combined titles like \"President/CEO.\"\n",
      "# Other variations could be handled as well, such\n",
      "# as \"President & CEO\", \"President and CEO\", etc.\n",
      "\n",
      "titles = []\n",
      "for contact in contacts:\n",
      "    titles.extend([t.strip() for t in contact['Job Title'].split('/')\n",
      "                  if contact['Job Title'].strip() != ''])\n",
      "\n",
      "# Replace common/known abbreviations\n",
      "\n",
      "for i, _ in enumerate(titles):\n",
      "    for transform in transforms:\n",
      "        titles[i] = titles[i].replace(*transform)\n",
      "\n",
      "# Print out a table of titles sorted by frequency\n",
      "\n",
      "pt = PrettyTable(field_names=['Title', 'Freq'])\n",
      "pt.align = 'l'\n",
      "c = Counter(titles)\n",
      "[pt.add_row([title, freq]) \n",
      " for (title, freq) in sorted(c.items(), key=itemgetter(1), reverse=True) \n",
      "     if freq > 0]\n",
      "print pt\n",
      "\n",
      "# Print out a table of tokens sorted by frequency\n",
      "\n",
      "tokens = []\n",
      "for title in titles:\n",
      "    tokens.extend([t.strip(',') for t in title.split()])\n",
      "pt = PrettyTable(field_names=['Token', 'Freq'])\n",
      "pt.align = 'l'\n",
      "c = Counter(tokens)\n",
      "[pt.add_row([token, freq]) \n",
      " for (token, freq) in sorted(c.items(), key=itemgetter(1), reverse=True) \n",
      "     if freq > 1 and len(token) > 2]\n",
      "print pt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+--------------------------------------+------+\n",
        "| Title                                | Freq |\n",
        "+--------------------------------------+------+\n",
        "| Software Engineer                    | 3    |\n",
        "| Database Administrator               | 1    |\n",
        "| Vice President Operations            | 1    |\n",
        "| Engineering Talent Manager           | 1    |\n",
        "| Design and Technology Coordinator    | 1    |\n",
        "| Certified Healthcare Navigator       | 1    |\n",
        "| Advisory Board Member                | 1    |\n",
        "|  Accounting Manager                  | 1    |\n",
        "| QA                                   | 1    |\n",
        "| Senior Director, Enterprise DevOps   | 1    |\n",
        "| Scientist                            | 1    |\n",
        "| Associate Quality Assurance Engineer | 1    |\n",
        "| Manager of a cleaning company        | 1    |\n",
        "| Advisor                              | 1    |\n",
        "| none                                 | 1    |\n",
        "| Developer                            | 1    |\n",
        "+--------------------------------------+------+\n",
        "+----------+------+\n",
        "| Token    | Freq |\n",
        "+----------+------+\n",
        "| Engineer | 4    |\n",
        "| Manager  | 3    |\n",
        "| Software | 3    |\n",
        "+----------+------+\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Assignment 3 Requirement##\n",
      "##OPTION 2 - Normalize professional titles to a minimum and print the updated list##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "from operator import itemgetter\n",
      "from collections import Counter\n",
      "from prettytable import PrettyTable\n",
      "\n",
      "# Place your \"Outlook CSV\" formatted file of connections from \n",
      "# http://www.linkedin.com/people/export-settings at the following location: \n",
      "# C:\\\\Users\\\\MisaelNMelissa\\\\Dropbox\\\\Mel_School\\\\Web 2.0 Architectures & Algorithms\\\\Homework\\\\Homework3_Serrano\\\\resources\\\\my_connections.csv\n",
      "\n",
      "CSV_FILE = \"resources\\\\my_connections.csv\"\n",
      "transforms = [\n",
      "    ('Sr.', 'Senior'),\n",
      "    ('Sr', 'Senior'),\n",
      "    ('Jr.', 'Junior'),\n",
      "    ('Jr', 'Junior'),\n",
      "    ('CEO', 'Chief Executive Officer'),\n",
      "    ('COO', 'Chief Operating Officer'),\n",
      "    ('CTO', 'Chief Technology Officer'),\n",
      "    ('CFO', 'Chief Finance Officer'),\n",
      "    ('VP', 'Vice President'),('Visiting', ''), ('Assistant', '')\n",
      "    ]\n",
      "# added the last two - some entries of professors with these qualifications...\n",
      "# much more refinement is needed!\n",
      "csvReader = csv.DictReader(open(CSV_FILE), delimiter=',', quotechar='\"')\n",
      "contacts = [row for row in csvReader]\n",
      "\n",
      "# Read in a list of titles and split apart\n",
      "# any combined titles like \"President/CEO.\"\n",
      "# Other variations could be handled as well, such\n",
      "# as \"President & CEO\", \"President and CEO\", etc.\n",
      "\n",
      "titles = []\n",
      "for contact in contacts:\n",
      "    titles.extend([t.strip() for t in contact['Job Title'].split('/')\n",
      "                  if contact['Job Title'].strip() != ''])\n",
      "\n",
      "# Replace common/known abbreviations\n",
      "\n",
      "for i, _ in enumerate(titles):\n",
      "    for transform in transforms:\n",
      "        titles[i] = titles[i].replace(*transform)\n",
      "\n",
      "#Find tokens of Job Titles and look for keywords which could be used as\n",
      "#the simplest form of a Job Title\n",
      "titles_new = [None] * len(titles)\n",
      "j = -1\n",
      "for title in titles:\n",
      "    j = j + 1\n",
      "    tokens = []\n",
      "    tokens.extend([t.strip(',') for t in title.split()])\n",
      "    for token in tokens:\n",
      "        if token == 'Manager':\n",
      "            titles_new[j] = 'Manager'\n",
      "            break\n",
      "        if token == 'Engineer':\n",
      "            titles_new[j] = 'Engineer'\n",
      "            break\n",
      "        if token == 'Developer':\n",
      "            titles_new[j] = 'Developer'\n",
      "            break\n",
      "        if token == 'Founder':\n",
      "            titles_new[j] = 'Founder'\n",
      "            break\n",
      "        if token == 'Student':\n",
      "            titles_new[j] = 'Student'\n",
      "            break\n",
      "        if token == 'Professor':\n",
      "            titles_new[j] = 'Professor'\n",
      "            break\n",
      "        if token == 'Vice':\n",
      "            titles_new[j] = 'Vice President'\n",
      "            break\n",
      "        if token == 'President':\n",
      "            titles_new[j] = 'President'\n",
      "            break        \n",
      "        if token == 'Director':\n",
      "            titles_new[j] = 'Director'\n",
      "            break        \n",
      "        if token == 'Administrator':\n",
      "            titles_new[j] = 'Administrator'\n",
      "            break\n",
      "        if token == 'Coordinator':\n",
      "            titles_new[j] = 'Coordinator'\n",
      "            break\n",
      "        if token == 'Member':\n",
      "            titles_new[j] = 'Member'\n",
      "            break\n",
      "        if token == 'Navigator':\n",
      "            titles_new[j] = 'Navigator'\n",
      "            break\n",
      "        else:\n",
      "            titles_new[j] = title\n",
      "        \n",
      "# Print out a table of titles sorted by frequency\n",
      "print titles_new\n",
      "pt = PrettyTable(field_names=['Simple Title', 'Freq'])\n",
      "pt.align = 'l'\n",
      "c = Counter(titles_new)\n",
      "[pt.add_row([titles_new, freq]) \n",
      " for (titles_new, freq) in sorted(c.items(), key=itemgetter(1), reverse=True) \n",
      "     if freq > 0]\n",
      "print pt\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['QA', 'Manager', 'Advisor', 'Engineer', 'Vice President', 'Manager', 'Director', 'Manager', 'Member', 'Engineer', 'Navigator', 'Scientist', 'Engineer', 'Coordinator', 'Administrator', 'Engineer', 'Developer', 'none']\n",
        "+----------------+------+\n",
        "| Simple Title   | Freq |\n",
        "+----------------+------+\n",
        "| Engineer       | 4    |\n",
        "| Manager        | 3    |\n",
        "| Director       | 1    |\n",
        "| Administrator  | 1    |\n",
        "| Vice President | 1    |\n",
        "| none           | 1    |\n",
        "| Member         | 1    |\n",
        "| QA             | 1    |\n",
        "| Coordinator    | 1    |\n",
        "| Scientist      | 1    |\n",
        "| Advisor        | 1    |\n",
        "| Navigator      | 1    |\n",
        "| Developer      | 1    |\n",
        "+----------------+------+\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Geocoding locations with Microsoft Bing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obtain a Microsoft Bing API Key from https://www.bingmapsportal.com and do a test query to obtain the geocode of a city.\n",
      "\n",
      "Before trying to access Bing Maps, be sure to run the following command in your Anaconda environment: \n",
      "<code> $ pip install geopy </code>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from geopy import geocoders\n",
      "\n",
      "GEO_APP_KEY = 'AoB3zLIJSUio83UFmF_Pz8QgPo2m-_PawYEHI4FWJFioI77YKjaCv4WLP7l3mKON' \n",
      "g = geocoders.Bing(GEO_APP_KEY)\n",
      "print g.geocode(\"Nashville\", exactly_one=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Location((36.1678390503, -86.7781600952, 0.0)), Location((39.2085494995, -86.2481384277, 0.0)), Location((38.3436813354, -89.3826370239, 0.0)), Location((33.9479293823, -93.847038269, 0.0)), Location((35.9743309021, -77.9649505615, 0.0))]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Geocoding locations of LinkedIn connections with Microsoft Bing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from geopy import geocoders\n",
      "\n",
      "GEO_APP_KEY = 'AoB3zLIJSUio83UFmF_Pz8QgPo2m-_PawYEHI4FWJFioI77YKjaCv4WLP7l3mKON'\n",
      "g = geocoders.Bing(GEO_APP_KEY)\n",
      "\n",
      "transforms = [('Greater ', ''), (' Area', '')]\n",
      "\n",
      "results = {}\n",
      "for c in connections['values']:\n",
      "    if not c.has_key('location'): continue\n",
      "        \n",
      "    transformed_location = c['location']['name']\n",
      "    for transform in transforms:\n",
      "        transformed_location = transformed_location.replace(*transform)\n",
      "    geo = g.geocode(transformed_location, exactly_one=False)\n",
      "    if geo == []: continue\n",
      "    results.update({ c['location']['name'] : geo })\n",
      "print results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'Orlando, Florida Area': [Location((28.538230896, -81.3773880005, 0.0))], u'United States': [Location((39.4432563782, -98.9573364258, 0.0))], u'Greater New York City Area': [Location((40.7820014954, -73.831703186, 0.0)), Location((40.7144317627, -74.0061035156, 0.0)), Location((40.7211990356, -73.9975967407, 0.0))], u'Wilmington, North Carolina Area': [Location((34.2349700928, -77.9459915161, 0.0))], u'Greensboro/Winston-Salem, North Carolina Area': [Location((35.254282, -80.794824, 0.0)), Location((36.079601, -79.746974, 0.0)), Location((35.063796, -78.886415, 0.0)), Location((35.088059, -78.904271, 0.0)), Location((35.834087, -80.242225, 0.0))], u'Albany, New York Area': [Location((42.651550293, -73.7552108765, 0.0))], u'Miami/Fort Lauderdale Area': [Location((26.089969635, -80.1362762451, 0.0))], u'Greater Milwaukee Area': [Location((43.041809082, -87.9068374634, 0.0))], u'West Palm Beach, Florida Area': [Location((26.714389801, -80.0531921387, 0.0))], u'Sacramento, California Area': [Location((38.4490699768, -121.343788147, 0.0))]}\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Parsing out states from Bing geocoder results using a regular expression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "# Most results contain a response that can be parsed by\n",
      "# picking out the first two consecutive upper case letters \n",
      "# as a clue for the state\n",
      "pattern = re.compile('.*([A-Z]{2}).*')\n",
      "    \n",
      "def parseStateFromBingResult(r):\n",
      "    result = pattern.search(r[0][0])\n",
      "    if result == None: \n",
      "        #print \"Unresolved match:\", r\n",
      "        return \"FL\" #normalizing it -- poorly!\n",
      "    elif len(result.groups()) == 1:\n",
      "        #print result.groups()\n",
      "        return result.groups()[0]\n",
      "    else:\n",
      "        #print \"Unresolved match:\", result.groups()\n",
      "        #return \"???\"\n",
      "        return \"FL\" #normalizing it -- poorly!\n",
      "    #Uncomment the two returns statements and show the results.\n",
      "    #Entries for United States and St.Paul confuse bing geocoder\n",
      "    \n",
      "transforms = [('Greater ', ''), (' Area', '')]\n",
      "\n",
      "results = {}\n",
      "for c in connections['values']:\n",
      "    if not c.has_key('location'): continue\n",
      "    if not c['location']['country']['code'] == 'us': continue\n",
      "        \n",
      "    transformed_location = c['location']['name']\n",
      "    for transform in transforms:\n",
      "        transformed_location = transformed_location.replace(*transform)\n",
      "    \n",
      "    geo = g.geocode(transformed_location, exactly_one=False)\n",
      "    if geo == []: continue\n",
      "    parsed_state = parseStateFromBingResult(geo)\n",
      "    if parsed_state != \"???\":\n",
      "        results.update({c['location']['name'] : parsed_state})\n",
      "    \n",
      "print json.dumps(results, indent=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\n",
        " \"Orlando, Florida Area\": \"FL\", \n",
        " \"United States\": \"FL\", \n",
        " \"Greater New York City Area\": \"NY\", \n",
        " \"Wilmington, North Carolina Area\": \"NC\", \n",
        " \"Greensboro/Winston-Salem, North Carolina Area\": \"NC\", \n",
        " \"Albany, New York Area\": \"NY\", \n",
        " \"Miami/Fort Lauderdale Area\": \"FL\", \n",
        " \"Greater Milwaukee Area\": \"WI\", \n",
        " \"West Palm Beach, Florida Area\": \"FL\", \n",
        " \"Sacramento, California Area\": \"CA\"\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Here's how to power a Cartogram visualization with the data**\n",
      "\n",
      "Count the number of connections in a state and save the frequency count to a json file.  Cartogram.html reads from the json file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import json\n",
      "from IPython.display import IFrame\n",
      "from IPython.display import HTML\n",
      "from IPython.core.display import display\n",
      "\n",
      "# Load in a data structure mapping state names to codes.\n",
      "# e.g. West Virginia is WV\n",
      "codes = json.loads(open('resources\\\\states_hash.json').read())\n",
      "\n",
      "from collections import Counter\n",
      "c = Counter([r[1] for r in results.items()])\n",
      "states_freqs = { codes[k] : v for (k,v) in c.items() }\n",
      "\n",
      "# Lace in all of the other states and provide a minimum value for each of them\n",
      "states_freqs.update({v : 0.1 for v in codes.values() if v not in states_freqs.keys() })\n",
      "\n",
      "# Write output to file\n",
      "f = open('states-freqs.json', 'w')\n",
      "f.write(json.dumps(states_freqs, indent=1))\n",
      "f.close()\n",
      "\n",
      "# IPython Notebook can serve files and display them into\n",
      "# inline frames. Prepend the path with the 'files' prefix\n",
      "#Display file in Iframe\n",
      "display(IFrame('states-freqs.json', '100%', '600px'))\n",
      "#Display cartogram in IFrame\n",
      "HTML('<iframe src=\"cartogram.html\" height=600 width=1050></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"100%\"\n",
        "            height=600px\"\n",
        "            src=\"states-freqs.json\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.lib.display.IFrame at 0x40297b8>"
       ]
      },
      {
       "html": [
        "<iframe src=\"cartogram.html\" height=600 width=1050></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "<IPython.core.display.HTML at 0x3df29b0>"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using NLTK to compute bigrams"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "se_bigrams = nltk.bigrams(\"Software Engineer\".split(), pad_right=True, \n",
      "                                                              pad_left=True)\n",
      "ad_bigrams = nltk.bigrams(\"Application Developer\".split(), pad_right=True, \n",
      "                                                               pad_left=True)\n",
      "\n",
      "print se_bigrams\n",
      "print ad_bigrams\n",
      "print len(set(se_bigrams).intersection(set(ad_bigrams)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<generator object bigrams at 0x0000000003BE1A68>\n",
        "<generator object bigrams at 0x00000000133E6288>\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Clustering job titles using a greedy heuristic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "from nltk.metrics.distance import jaccard_distance\n",
      "\n",
      "# XXX: Place your \"Outlook CSV\" formatted file of connections from \n",
      "# http://www.linkedin.com/people/export-settings \n",
      "CSV_FILE = 'resources\\\\my_connections.csv'\n",
      "# Tweak this distance threshold and try different distance calculations \n",
      "# during experimentation\n",
      "DISTANCE_THRESHOLD = 0.5\n",
      "DISTANCE = jaccard_distance\n",
      "\n",
      "def cluster_contacts_by_title(csv_file):\n",
      "\n",
      "    transforms = [\n",
      "        ('Sr.', 'Senior'),\n",
      "        ('Sr', 'Senior'),\n",
      "        ('Jr.', 'Junior'),\n",
      "        ('Jr', 'Junior'),\n",
      "        ('CEO', 'Chief Executive Officer'),\n",
      "        ('COO', 'Chief Operating Officer'),\n",
      "        ('CTO', 'Chief Technology Officer'),\n",
      "        ('CFO', 'Chief Finance Officer')\n",
      "        ,('VP', 'Vice President'),\n",
      "        ]\n",
      "\n",
      "    separators = ['/', ' and ', '&', ',']\n",
      "\n",
      "    csvReader = csv.DictReader(open(csv_file), delimiter=',', quotechar='\"')\n",
      "    contacts = [row for row in csvReader]\n",
      "\n",
      "    # Normalize and/or replace known abbreviations\n",
      "    # and build up a list of common titles.\n",
      "\n",
      "    all_titles = []\n",
      "    for i, _ in enumerate(contacts):\n",
      "        if contacts[i]['Job Title'] == '':\n",
      "            contacts[i]['Job Titles'] = ['']\n",
      "            continue\n",
      "        titles = [contacts[i]['Job Title'].strip()]\n",
      "        for title in titles:\n",
      "            #print title\n",
      "            for separator in separators:\n",
      "                if title.find(separator) >= 0:\n",
      "                    titles.remove(title.strip())\n",
      "                    titles.extend([title.strip() for title in title.split(separator)\n",
      "                                  if title.strip() != ''])\n",
      "\n",
      "        for transform in transforms:\n",
      "            titles = [title.replace(*transform) for title in titles]\n",
      "           # print titles\n",
      "        contacts[i]['Job Titles'] = titles\n",
      "        all_titles.extend(titles)\n",
      "\n",
      "    all_titles = list(set(all_titles))\n",
      "    print all_titles\n",
      "\n",
      "    clusters = {}\n",
      "    for title1 in all_titles:\n",
      "        clusters[title1] = []\n",
      "        #print clusters[title1]\n",
      "        for title2 in all_titles:\n",
      "           # print 'title1:' + title1 + ', title2: ' + title2\n",
      "            if title2 in clusters[title1] or clusters.has_key(title2) and title1 \\\n",
      "                in clusters[title2]:\n",
      "                #print clusters[title1], clusters[title2]\n",
      "                continue\n",
      "            distance = DISTANCE(set(title1.split()), set(title2.split()))\n",
      "\n",
      "            if distance < DISTANCE_THRESHOLD:\n",
      "                clusters[title1].append(title2)\n",
      "                #print clusters[title1]\n",
      "\n",
      "    # Flatten out clusters\n",
      "\n",
      "    clusters = [clusters[title] for title in clusters if len(clusters[title]) > 1]\n",
      "\n",
      "    # Round up contacts who are in these clusters and group them together\n",
      "\n",
      "    clustered_contacts = {}\n",
      "    for cluster in clusters:\n",
      "        clustered_contacts[tuple(cluster)] = []\n",
      "        for contact in contacts:\n",
      "            for title in contact['Job Titles']:\n",
      "                if title in cluster:\n",
      "                    clustered_contacts[tuple(cluster)].append('%s %s'\n",
      "                            % (contact['First Name'], contact['Last Name']))\n",
      "\n",
      "    return clustered_contacts\n",
      "    #print clustered_contacts\n",
      "\n",
      "\n",
      "clustered_contacts = cluster_contacts_by_title(CSV_FILE)\n",
      "print clustered_contacts  # execute till here and show the results before grouping\n",
      "for titles in clustered_contacts:\n",
      "    print titles\n",
      "    common_titles_heading = 'Common Titles: ' + ', '.join(titles)\n",
      "\n",
      "    descriptive_terms = set(titles[0].split())\n",
      "    for title in titles:\n",
      "        descriptive_terms.intersection_update(set(title.split()))\n",
      "    descriptive_terms_heading = 'Descriptive Terms: ' \\\n",
      "        + ', '.join(descriptive_terms)\n",
      "    print descriptive_terms_heading\n",
      "    print '-' * max(len(descriptive_terms_heading), len(common_titles_heading))\n",
      "    print '\\n'.join(clustered_contacts[titles])\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Application Developer', 'Assistant Accounting Manager', 'Senior Director', 'Software Engineer Intern', 'Vice President Operations', 'Engineering Talent Manager', 'Enterprise DevOps', 'Advisory Board Member', 'Database Administrator', 'QA', 'Technology Coordinator', 'Scientist', 'Design', 'Associate Quality Assurance Engineer', 'Manager of a cleaning company', 'Advisor', 'none', 'Associate Software Engineer', 'Software Development Engineer', 'Certified Healthcare Navigator']\n",
        "{}\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Incorporating random sampling can improve performance**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "import random\n",
      "from nltk.metrics.distance import jaccard_distance\n",
      "\n",
      "# XXX: Place your \"Outlook CSV\" formatted file of connections from \n",
      "# http://www.linkedin.com/people/export-settings\n",
      "CSV_FILE = 'resources\\\\my_connections.csv'\n",
      "\n",
      "# Tweak this distance threshold and try different distance calculations \n",
      "# during experimentation\n",
      "DISTANCE_THRESHOLD = 0.5\n",
      "DISTANCE = jaccard_distance\n",
      "\n",
      "# Adjust sample size as needed to reduce the runtime of the\n",
      "# nested loop that invokes the DISTANCE function\n",
      "SAMPLE_SIZE = 50\n",
      "\n",
      "def cluster_contacts_by_title(csv_file):\n",
      "\n",
      "    transforms = [\n",
      "        ('Sr.', 'Senior'),\n",
      "        ('Sr', 'Senior'),\n",
      "        ('Jr.', 'Junior'),\n",
      "        ('Jr', 'Junior'),\n",
      "        ('CEO', 'Chief Executive Officer'),\n",
      "        ('COO', 'Chief Operating Officer'),\n",
      "        ('CTO', 'Chief Technology Officer'),\n",
      "        ('CFO', 'Chief Finance Officer'),\n",
      "        ('VP', 'Vice President'),\n",
      "        ]\n",
      "\n",
      "    separators = ['/', ' and ', '&', ',']\n",
      "\n",
      "    csvReader = csv.DictReader(open(csv_file), delimiter=',', quotechar='\"')\n",
      "    contacts = [row for row in csvReader]\n",
      "\n",
      "    # Normalize and/or replace known abbreviations\n",
      "    # and build up list of common titles\n",
      "\n",
      "    all_titles = []\n",
      "    for i, _ in enumerate(contacts):\n",
      "        if contacts[i]['Job Title'] == '':\n",
      "            contacts[i]['Job Titles'] = ['']\n",
      "            continue\n",
      "        titles = [contacts[i]['Job Title'].strip()]\n",
      "        for title in titles:\n",
      "            for separator in separators:\n",
      "                if title.find(separator) >= 0:\n",
      "                    titles.remove(title.strip())\n",
      "                    titles.extend([title.strip() for title in title.split(separator)\n",
      "                                  if title.strip() != ''])\n",
      "\n",
      "        for transform in transforms:\n",
      "            titles = [title.replace(*transform) for title in titles]\n",
      "        contacts[i]['Job Titles'] = titles\n",
      "        all_titles.extend(titles)\n",
      "\n",
      "    all_titles = list(set(all_titles))\n",
      "    print all_titles\n",
      "    clusters = {}\n",
      "    for title1 in all_titles:\n",
      "        clusters[title1] = []\n",
      "        for sample in xrange(SAMPLE_SIZE):\n",
      "            title2 = all_titles[random.randint(0, len(all_titles)-1)]\n",
      "            if title2 in clusters[title1] or clusters.has_key(title2) and title1 \\\n",
      "                in clusters[title2]:\n",
      "                continue\n",
      "            distance = DISTANCE(set(title1.split()), set(title2.split()))\n",
      "            if distance < DISTANCE_THRESHOLD:\n",
      "                clusters[title1].append(title2)\n",
      "\n",
      "    # Flatten out clusters\n",
      "\n",
      "    clusters = [clusters[title] for title in clusters if len(clusters[title]) > 1]\n",
      "\n",
      "    # Round up contacts who are in these clusters and group them together\n",
      "\n",
      "    clustered_contacts = {}\n",
      "    for cluster in clusters:\n",
      "        clustered_contacts[tuple(cluster)] = []\n",
      "        for contact in contacts:\n",
      "            for title in contact['Job Titles']:\n",
      "                if title in cluster:\n",
      "                    clustered_contacts[tuple(cluster)].append('%s %s'\n",
      "                            % (contact['First Name'], contact['Last Name']))\n",
      "\n",
      "    return clustered_contacts\n",
      "\n",
      "\n",
      "clustered_contacts = cluster_contacts_by_title(CSV_FILE)\n",
      "print clustered_contacts\n",
      "for titles in clustered_contacts:\n",
      "    common_titles_heading = 'Common Titles: ' + ', '.join(titles)\n",
      "\n",
      "    descriptive_terms = set(titles[0].split())\n",
      "    for title in titles:\n",
      "        descriptive_terms.intersection_update(set(title.split()))\n",
      "    descriptive_terms_heading = 'Descriptive Terms: ' \\\n",
      "        + ', '.join(descriptive_terms)\n",
      "    print descriptive_terms_heading\n",
      "    print '-' * max(len(descriptive_terms_heading), len(common_titles_heading))\n",
      "    print '\\n'.join(clustered_contacts[titles])\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Application Developer', 'Assistant Accounting Manager', 'Senior Director', 'Software Engineer Intern', 'Vice President Operations', 'Engineering Talent Manager', 'Enterprise DevOps', 'Advisory Board Member', 'Database Administrator', 'QA', 'Technology Coordinator', 'Scientist', 'Design', 'Associate Quality Assurance Engineer', 'Manager of a cleaning company', 'Advisor', 'none', 'Associate Software Engineer', 'Software Development Engineer', 'Certified Healthcare Navigator']\n",
        "{}\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**How to export data (contained in the \"clustered contacts\" variable) to display a folder tree structure**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import os\n",
      "from IPython.display import IFrame\n",
      "from IPython.display import HTML\n",
      "from IPython.core.display import display\n",
      "\n",
      "data = {\"label\" : \"name\", \"temp_items\" : {}, \"items\" : []} \n",
      "for titles in clustered_contacts:\n",
      "    descriptive_terms = set(titles[0].split())\n",
      "    for title in titles:\n",
      "        descriptive_terms.intersection_update(set(title.split()))\n",
      "    descriptive_terms = ', '.join(descriptive_terms)\n",
      "\n",
      "    if data['temp_items'].has_key(descriptive_terms):\n",
      "        data['temp_items'][descriptive_terms].extend([{'name' : cc } for cc \n",
      "            in clustered_contacts[titles]])\n",
      "    else:\n",
      "        data['temp_items'][descriptive_terms] = [{'name' : cc } for cc \n",
      "            in clustered_contacts[titles]]\n",
      "\n",
      "for descriptive_terms in data['temp_items']:\n",
      "    data['items'].append({\"name\" : \"%s (%s)\" % (descriptive_terms, \n",
      "        len(data['temp_items'][descriptive_terms]),),\n",
      "                              \"children\" : [i for i in \n",
      "                              data['temp_items'][descriptive_terms]]})\n",
      "\n",
      "del data['temp_items']\n",
      "\n",
      "TEMPLATE = 'dojo_tree.html.template' \n",
      "OUT = 'dojo_tree.html'\n",
      "\n",
      "#viz_file = 'files/resources/ch03-linkedin/viz/dojo_tree.html'\n",
      "#viz_file = ''#THIS IS YET TO BE FILLED IN\n",
      "t = open(TEMPLATE).read()\n",
      "f = open(OUT, 'w')\n",
      "#f.write(t % json.dumps(data, indent=4))\n",
      "f.write(json.dumps(data, indent=4))\n",
      "f.close()\n",
      "\n",
      "# IPython Notebook can serve files and display them into\n",
      "# inline frames. Prepend the path with the 'files' prefix\n",
      "HTML('<iframe src=\"dojo_tree.html.template\" height=600 width=1050></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe src=\"dojo_tree.html.template\" height=600 width=1050></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "<IPython.core.display.HTML at 0x133e8b38>"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**How to export data to power a dendogram and node-link tree visualization**\n",
      "\n",
      "Save the clustering structure of the connections to a json file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "import random\n",
      "from nltk.metrics.distance import jaccard_distance\n",
      "from cluster import HierarchicalClustering\n",
      "\n",
      "# XXX: Place your \"Outlook CSV\" formatted file of connections from \n",
      "# http://www.linkedin.com/people/export-settings at the following\n",
      "\n",
      "CSV_FILE = 'resources\\\\my_connections.csv'\n",
      "OUT_FILE = 'd3-data.json'\n",
      "# Tweak this distance threshold and try different distance calculations \n",
      "# during experimentation\n",
      "DISTANCE_THRESHOLD = 0.5\n",
      "DISTANCE = jaccard_distance\n",
      "\n",
      "# Adjust sample size as needed to reduce the runtime of the\n",
      "# nested loop that invokes the DISTANCE function\n",
      "SAMPLE_SIZE = 500\n",
      "\n",
      "def cluster_contacts_by_title(csv_file):\n",
      "\n",
      "    transforms = [\n",
      "        ('Sr.', 'Senior'),\n",
      "        ('Sr', 'Senior'),\n",
      "        ('Jr.', 'Junior'),\n",
      "        ('Jr', 'Junior'),\n",
      "        ('CEO', 'Chief Executive Officer'),\n",
      "        ('COO', 'Chief Operating Officer'),\n",
      "        ('CTO', 'Chief Technology Officer'),\n",
      "        ('CFO', 'Chief Finance Officer'),\n",
      "        ('VP', 'Vice President'),\n",
      "        ]\n",
      "\n",
      "    separators = ['/', 'and', '&', '-', '|'] # added the - and | after reviewing the csv file\n",
      "\n",
      "    csvReader = csv.DictReader(open(csv_file), delimiter=',', quotechar='\"')\n",
      "    contacts = [row for row in csvReader]\n",
      "\n",
      "    # Normalize and/or replace known abbreviations\n",
      "    # and build up list of common titles\n",
      "\n",
      "    all_titles = []\n",
      "    for i, _ in enumerate(contacts):\n",
      "        if contacts[i]['Job Title'] == '':\n",
      "            contacts[i]['Job Titles'] = ['']\n",
      "            continue\n",
      "        titles = [contacts[i]['Job Title']]\n",
      "        print titles # my addition\n",
      "        # found that one guy used both \\ and - in his/her job title\n",
      "        # I physically shortened it to get the code to run!\n",
      "        for title in titles:\n",
      "            for separator in separators:\n",
      "                if title.find(separator) >= 0:\n",
      "                    titles.remove(title)\n",
      "                    titles.extend([title.strip() for title in title.split(separator)\n",
      "                                  if title.strip() != ''])\n",
      "\n",
      "        for transform in transforms:\n",
      "            titles = [title.replace(*transform) for title in titles]\n",
      "        contacts[i]['Job Titles'] = titles\n",
      "        all_titles.extend(titles)\n",
      "\n",
      "    all_titles = list(set(all_titles))\n",
      "    \n",
      "    # Define a scoring function\n",
      "    def score(title1, title2): \n",
      "        return DISTANCE(set(title1.split()), set(title2.split()))\n",
      "\n",
      "    # Feed the class your data and the scoring function\n",
      "    hc = HierarchicalClustering(all_titles, score)\n",
      "\n",
      "    # Cluster the data according to a distance threshold\n",
      "    clusters = hc.getlevel(DISTANCE_THRESHOLD)\n",
      "\n",
      "    # Remove singleton clusters\n",
      "    clusters = [c for c in clusters if len(c) > 1]\n",
      "\n",
      "    # Round up contacts who are in these clusters and group them together\n",
      "\n",
      "    clustered_contacts = {}\n",
      "    for cluster in clusters:\n",
      "        clustered_contacts[tuple(cluster)] = []\n",
      "        for contact in contacts:\n",
      "            for title in contact['Job Titles']:\n",
      "                if title in cluster:\n",
      "                    clustered_contacts[tuple(cluster)].append('%s %s'\n",
      "                            % (contact['First Name'], contact['Last Name']))\n",
      "\n",
      "    return clustered_contacts\n",
      "\n",
      "def display_output(clustered_contacts):\n",
      "    \n",
      "    for titles in clustered_contacts:\n",
      "        common_titles_heading = 'Common Titles: ' + ', '.join(titles)\n",
      "\n",
      "        descriptive_terms = set(titles[0].split())\n",
      "        for title in titles:\n",
      "            descriptive_terms.intersection_update(set(title.split()))\n",
      "        descriptive_terms_heading = 'Descriptive Terms: ' \\\n",
      "            + ', '.join(descriptive_terms)\n",
      "        print descriptive_terms_heading\n",
      "        print '-' * max(len(descriptive_terms_heading), len(common_titles_heading))\n",
      "        print '\\n'.join(clustered_contacts[titles])\n",
      "        print\n",
      "\n",
      "def write_d3_json_output(clustered_contacts):\n",
      "    \n",
      "    json_output = {'name' : 'My LinkedIn', 'children' : []}\n",
      "\n",
      "    for titles in clustered_contacts:\n",
      "\n",
      "        descriptive_terms = set(titles[0].split())\n",
      "        for title in titles:\n",
      "            descriptive_terms.intersection_update(set(title.split()))\n",
      "\n",
      "        json_output['children'].append({'name' : ', '.join(descriptive_terms)[:30], \n",
      "                                    'children' : [ {'name' : c.decode('utf-8', 'replace')} for c in clustered_contacts[titles] ] } )\n",
      "    \n",
      "        f = open(OUT_FILE, 'w')\n",
      "        f.write(json.dumps(json_output, indent=1))\n",
      "        f.close()\n",
      "    \n",
      "clustered_contacts = cluster_contacts_by_title(CSV_FILE)\n",
      "display_output(clustered_contacts)\n",
      "write_d3_json_output(clustered_contacts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['QA']\n",
        "['Assistant Accounting Manager']\n",
        "['Advisor']\n",
        "['Associate Quality Assurance Engineer']\n",
        "['VP Operations']\n",
        "['Engineering Talent Manager']\n",
        "['Sr. Director, Enterprise DevOps']\n",
        "['Manager of a cleaning company']\n",
        "['Advisory Board Member']\n",
        "['Associate Software Engineer']\n",
        "['Certified Healthcare Navigator']\n",
        "['Scientist']\n",
        "['Software Development Engineer']\n",
        "['Design and Technology Coordinator']\n",
        "['Database Administrator']\n",
        "['Software Engineer Intern']\n",
        "['Application Developer']\n",
        "['none']\n",
        "Descriptive Terms: Software, Engineer\n",
        "---------------------------------------------------------------------------------------------------\n",
        "Jimmy Mauri\n",
        "Jebin Shakya\n",
        "David Vizcaino\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Using the json output file, we can display the data in a node link tree**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from IPython.display import IFrame\n",
      "from IPython.display import HTML\n",
      "from IPython.core.display import display\n",
      "\n",
      "# IPython Notebook can serve files and display them into\n",
      "# inline frames. Prepend the path with the 'files' prefix\n",
      "\n",
      "HTML('<iframe src=\"node_link_tree.html\" height=600 width=1050></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe src=\"node_link_tree.html\" height=600 width=1050></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "<IPython.core.display.HTML at 0x13489fd0>"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Using the json output file, we can display the data in a dendogram**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from IPython.display import IFrame\n",
      "from IPython.display import HTML\n",
      "from IPython.core.display import display\n",
      "\n",
      "# IPython Notebook can serve files and display them into\n",
      "# inline frames. Prepend the path with the 'files' prefix\n",
      "\n",
      "HTML('<iframe src=\"dendogram.html\" height=900 width=1050></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe src=\"dendogram.html\" height=900 width=1050></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "<IPython.core.display.HTML at 0x134897f0>"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Clustering your LinkedIn professional network based upon the locations of your connections and emitting KML output for visualization with Google Earth"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**I exported the LinkedIn Connection Data to a KML file.  I created a Google Map under My Maps in my Google account and imported the\n",
      "KML file as a layer.  I had to share the link and allow it to be public, and get the link from here as well for the iframe to work.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import json\n",
      "from IPython.display import HTML\n",
      "import webbrowser\n",
      "#import re #my addition\n",
      "from urllib2 import HTTPError\n",
      "from geopy import geocoders\n",
      "#from cluster import KMeansClustering, centroid  #-- original code\n",
      "from cluster import KMeansClustering\n",
      "from scipy import cluster\n",
      "#, cluster # centroid is under cluster.hierarchy --was not called properly\n",
      "# see http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.centroid.html\n",
      "# A helper function to munge data and build up an XML tree.\n",
      "from linkedin__kml_utility import createKML\n",
      "\n",
      "# XXX: Try different values for K to see the difference in clusters that emerge\n",
      "K = 3\n",
      "\n",
      "# XXX: Get an API key and pass it in here. See https://www.bingmapsportal.com.\n",
      "GEO_API_KEY = 'AoB3zLIJSUio83UFmF_Pz8QgPo2m-_PawYEHI4FWJFioI77YKjaCv4WLP7l3mKON'\n",
      "g = geocoders.Bing(GEO_API_KEY)\n",
      "\n",
      "# Load this data from where you've previously stored it\n",
      "CONNECTIONS_DATA = 'resources\\\\connections_copy.json'\n",
      "\n",
      "OUT_FILE = \"linkedin_clusters_kmeans.kml\"\n",
      "\n",
      "# Open up your saved connections with extended profile information\n",
      "# or fetch them again from LinkedIn if you prefer\n",
      "\n",
      "connections = json.loads(open(CONNECTIONS_DATA).read())['values']\n",
      "\n",
      "locations = [c['location']['name'] for c in connections if c.has_key('location')]\n",
      "#print locations #my first addition  -- lot of \\ separated entries -- Ft. Laud\\Miami etc - OK\n",
      "# Some basic transforms may be necessary for geocoding services to function properly\n",
      "# Here are a couple that seem to help.\n",
      "\n",
      "#transforms = [('Greater ', ''), (' Area', '')]\n",
      "transforms = [('Greater ', '')]# my fix to keep things moving\n",
      "# Step 1 - Tally the frequency of each location\n",
      "\n",
      "coords_freqs = {}\n",
      "for location in locations:\n",
      "\n",
      "    if not c.has_key('location'): continue\n",
      "    \n",
      "    # Avoid unnecessary I/O and geo requests by building up a cache\n",
      "\n",
      "    if coords_freqs.has_key(location):\n",
      "        coords_freqs[location][1] += 1\n",
      "        continue\n",
      "    transformed_location = location\n",
      "    transformed_location = transformed_location.replace(',','') # did not help with West palm beach\n",
      "    #print transformed_location #my 3rd addition - after removing the comma\n",
      "    for transform in transforms:\n",
      "        #print transformed_location #my 3rd addition - before xform  \n",
      "        #transformed_location = transformed_location.replace(*transform)-- original code\n",
      "        transformed_location = transformed_location.replace('Greater ', '')\n",
      "        transformed_location = transformed_location.replace(' Area', '')\n",
      "        transformed_location = transformed_location.replace(' Masovian District', '')# ran code \n",
      "        # to find this error and fixed it thus\n",
      "        print transformed_location #my 3rd addition - after xform\n",
      "        # It is quitting after the first entry of Cochin Area, India\n",
      "        # removed Area from that entry -- forced normalization\n",
      "        # did the same for the second one: Wichita, Kansas Area\n",
      "        # It processed it OK for Greater San Diego  Area\n",
      "        # but failed for West Palm Beach, Florida Area\n",
      "        # Clearly the comma is interfering with the operation of the replace operation\n",
      "        # Handle potential I/O errors with a retry pattern...\n",
      "        \n",
      "        while True:\n",
      "            num_errors = 0\n",
      "            try:\n",
      "                results = g.geocode(transformed_location, exactly_one=False)\n",
      "                print results  # my third addition  - does  not work oK here\n",
      "                break\n",
      "            except HTTPError, e:\n",
      "                num_errors += 1\n",
      "                if num_errors >= 3:\n",
      "                    sys.exit()\n",
      "                print >> sys.stderr, e\n",
      "                print >> sys.stderr, 'Encountered an urllib2 error. Trying again...'\n",
      "                \n",
      "        for result in results:\n",
      "            #print result # my second addition - verified entries are ok above with print results\n",
      "            # prints none here - so, after 'cochin Area, India' it was quitting\n",
      "            # So, I removed Area from that entry; then it got stuck at Wichita, Kansas Area, \n",
      "            # A check above shows that *transform is not working - it works in earlier step\n",
      "            # I do know there are entries that may be causing trouble.....\n",
      "            # Each result is of the form (\"Description\", (X,Y))\n",
      "            #BOTTOM LINE: 'Area' is not getting removed, even after the comma in between was removed\n",
      "            coords_freqs[location] = [result[1], 1]\n",
      "            break # Disambiguation strategy is \"pick first\"\n",
      "\n",
      "# Step 2 - Build up data structure for converting locations to KML            \n",
      "            \n",
      "# Here, you could optionally segment locations by continent or country\n",
      "# so as to avoid potentially finding a mean in the middle of the ocean.\n",
      "# The k-means algorithm will expect distinct points for each contact, so\n",
      "# build out an expanded list to pass it.\n",
      "\n",
      "expanded_coords = []\n",
      "for label in coords_freqs:\n",
      "    # Flip lat/lon for Google Earth\n",
      "    ((lat, lon), f) = coords_freqs[label]\n",
      "    expanded_coords.append((label, [(lon, lat)] * f))\n",
      "\n",
      "# No need to clutter the map with unnecessary placemarks...\n",
      "kml_items = [{'label': label, 'coords': '%s,%s' % coords[0]} for (label,\n",
      "             coords) in expanded_coords]\n",
      "\n",
      "# It would also be helpful to include names of your contacts on the map\n",
      "for item in kml_items:\n",
      "    item['contacts'] = '\\n'.join(['%s %s.' % (c['firstName'], c['lastName'])\n",
      "        for c in connections if c.has_key('location') and \n",
      "                                c['location']['name'] == item['label']])\n",
      "\n",
      "# Step 3 - Cluster locations and extend the KML data structure with centroids\n",
      "cl = KMeansClustering([coords for (label, coords_list) in expanded_coords\n",
      "                      for coords in coords_list])\n",
      "#print cl.getclusters(K)-- OK\n",
      "print \"------0 below----\"\n",
      "print cl.getclusters(K)[0]\n",
      "print \"---1 below ------ \"\n",
      "print cl.getclusters(K)[1]\n",
      "print \"---2 below ------ \"\n",
      "print cl.getclusters(K)[2]\n",
      "\n",
      "centroids = [{'label': 'CENTROID', 'coords': '%s,%s'} for c in\n",
      "             cl.getclusters(K)]\n",
      "\n",
      "kml_items.extend(centroids)\n",
      "\n",
      "# Step 4 - Create the final KML output and write it to a file\n",
      "kml = createKML(kml_items)\n",
      "\n",
      "f = open(OUT_FILE, 'w')\n",
      "f.write(kml)\n",
      "f.close()\n",
      "\n",
      "print 'Data written to ' + OUT_FILE\n",
      "#Open Google Map with KML overlay in new window\n",
      "webbrowser.open_new('https://www.google.com/maps/d/edit?mid=zF3yTGj-asBs.kZh0YMDcFwA8')\n",
      "#iFrame linked obtained from My Google Maps\n",
      "HTML('<iframe src=\"https://www.google.com/maps/d/embed?mid=zF3yTGj-asBs.kZh0YMDcFwA8\" width=\"640\" height=\"480\"></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "West Palm Beach Florida\n",
        "[Location((26.714389801, -80.0531921387, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Orlando Florida\n",
        "[Location((28.538230896, -81.3773880005, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Miami/Fort Lauderdale\n",
        "[Location((26.089969635, -80.1362762451, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "New York City\n",
        "[Location((40.7820014954, -73.831703186, 0.0)), Location((40.7144317627, -74.0061035156, 0.0)), Location((40.7211990356, -73.9975967407, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Greensboro/Winston-Salem North Carolina\n",
        "[Location((35.254282, -80.794824, 0.0)), Location((36.079601, -79.746974, 0.0)), Location((35.063796, -78.886415, 0.0)), Location((35.088059, -78.904271, 0.0)), Location((35.834087, -80.242225, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Milwaukee\n",
        "[Location((43.041809082, -87.9068374634, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Albany New York\n",
        "[Location((42.651550293, -73.7552108765, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Sacramento California\n",
        "[Location((38.4490699768, -121.343788147, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wilmington North Carolina\n",
        "[Location((34.2349700928, -77.9459915161, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "United States\n",
        "[Location((39.4432563782, -98.9573364258, 0.0))]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "------0 below----\n",
        "[(-80.05319213867188, 26.71438980102539), (-80.05319213867188, 26.71438980102539), (-80.05319213867188, 26.71438980102539), (-80.05319213867188, 26.71438980102539), (-80.05319213867188, 26.71438980102539), (-81.37738800048828, 28.538230895996094), (-80.05319213867188, 26.71438980102539), (-80.05319213867188, 26.71438980102539)]\n",
        "---1 below ------ \n",
        "[(-87.9068374633789, 43.04180908203125), (-121.34378814697266, 38.44906997680664), (-77.94599151611328, 34.23497009277344), (-73.83170318603516, 40.78200149536133), (-80.794824, 35.254282), (-98.95733642578125, 39.44325637817383), (-73.75521087646484, 42.65155029296875)]\n",
        "---2 below ------ \n",
        "[(-80.13627624511719, 26.089969635009766), (-80.13627624511719, 26.089969635009766), (-80.13627624511719, 26.089969635009766), (-80.13627624511719, 26.089969635009766), (-80.13627624511719, 26.089969635009766), (-80.13627624511719, 26.089969635009766), (-80.13627624511719, 26.089969635009766)]\n",
        "Data written to linkedin_clusters_kmeans.kml\n"
       ]
      },
      {
       "html": [
        "<iframe src=\"https://www.google.com/maps/d/embed?mid=zF3yTGj-asBs.kZh0YMDcFwA8\" width=\"640\" height=\"480\"></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "<IPython.core.display.HTML at 0x136d6c50>"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}